FreeEed summary
===============

TO RUN THE PROGRAM
==================

Use Ubuntu, unzip the distribution, go to the distribution directory and
on the command line enter the following

java -jar dist/FreeEed.jar

This will tell you the list of options and you are ready to go!

The two most important things about FreeEed:

1. It really works and can process data, make it searchable, and
output results as a CSV files, with native documents in a zip file.
No MS Office install is needed, although Ubuntu Linux is recommended.

2. It is scalable: if you have tens or hundreds of machines 
and have set up a Hadoop cluster on them, it will work using
all these machines, dividing the load and processing all the data.
The same code will work on one machine or on hundreds.

Purpose of the project
======================

The purpose of the Free Eed - Free Electronic Evidence Discovery Project - is to 
lay down the foundation of a correctly architected system for eDiscovery, one 
that will be scalable to process and store any amounts of electronic information, 
and flexible enough to accommodate disparate sources of data collection, both now 
and in the future.

The basic philosophy of FreeEed is “make it work.” While commercial eDiscovery 
providers have to accommodate every possible option, such as MSG production, 
for example, the FreeEed, by contrast, is good enough if it allows eDiscovery 
to proceed. People can add options and extensions, if they want.

The basic building blocks of the system are HDFS, Hadoop, HBase/SimpleDB, Tika, 
Lucene, and Solr. These are the obvious choices for any crawler, and eDiscovery 
crawler is similar enough to any web crawler, so that whatever worked for 
Google and Nutch, should be applicable for FreeEed also.

FreeEed is open-sourced under the Apache 2.0 License. This is the 
same license as used by Hadoop, HBase, Tika, and Lucene. 
It allows the same usage as these popular software packages, but stops others
from close-sourcing it.

You can deploy FreeEed in both private Hadoop clusters and on Amazon EC2.

The project is hosted on GitHub and its home is on freeeed.org 

The architecture of the system is straightforward.
 
Staging. All files that require processing are packaged into archives and 
uploaded to HDFS/S3. This approaches avoids problems with too many files in 
HDFS and follows the approach outlined by Sierra: 
http://stuartsierra.com/2008/04/24/a-million-little-files File system metadata 
(custodian, dates, etc.) are packaged into the archive together with the file. 
This is needed for processing and opens a way for future integration 
with forensics investigations;

Processing. Processing is organized by the Hadoop framework. 
Each file is read from the archive, is assigned a permanent id using 
HBase/SimpleDB, and processed with Tika, which extracts text and metadata. 
Metadata, text, and the file itself are stored in HBase/SimpleDB;

Indexing. Each project creates its own Lucene index for later searches. 
Large projects may need to use Katta for scalability.

Output. Metadata results are output in CSV file, while the native files and 
the extracted text are stored in a zip file(s).

The end results can be used for culling and producing native files for 
legal review. For this, FreeEed will use the site crocodoc.com and 
allow bulk load through their API.

Current capabilities

* process multiple input directories
* work locally or on a Hadoop cluster
* output metadata in CSV as a load file
* supported file formats - all Tika-supported formats, http://tika.apache.org/0.9/formats.html

Future 

* process PST emails
* private Lucene index for searches
* allow searching and culling through the browser interface
* whatever the users of FreeEed will suggest